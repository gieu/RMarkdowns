
---
title: "Unsupervised ML Algorithms"
author: "Camilo Vieira, Ph.D., cvieira@uninorte.edu.co"
output: learnr::tutorial
runtime: shiny_prerendered
---

```{r setup, include=FALSE} 
library(learnr)
library(Hmisc)
library(ggalluvial)
#knitr::opts_chunk$set(echo = FALSE)
groups<- kmeans(iris[,c('Sepal.Length','Petal.Length')], 2)
computeDistance<- function(x, randomCentroids){
  dataPoint<- as.numeric(x[c(1,3)])
  distance<-c(norm(as.matrix(dataPoint-randomCentroids[1,]),"1")^2,
              norm(as.matrix(dataPoint-randomCentroids[2,]),"1")^2,
              norm(as.matrix(dataPoint-randomCentroids[3,]),"1")^2)
  which.min(distance)
}
```

## K-Meas

#### Usage-Rigts
These materials were created for educational purposes. They are licensed under creative commons. 

```{r, echo=FALSE}
htmltools::includeHTML("license.html")
```


#### Learning outcomes
By the end of this activity, you will be able to: 

* Use R to identify clusters in a data set using the kmeans method

* Create simple plots to depict the clusters in a data set

### K-means

K-means is an unsupervised machine learning algorithm that takes a N-dimensional data set and mimizes a distance measure for partitioning the data set into *k* groups or clusters. The distance measure is the *sum of squared Euclidean distances* between the data points and the. 

#### Procedure (From Dr. Sanchez-Pena's slides)

1. Initially, you randomly pick *k* centroids (or points that will be the center of your clusters) in *d-space*. Try to make them near the data but different from one another.

2. Then assign each data point to the closest centroid through a distance measurement.

3. Move the centroids to the average location of the data points (which correspond to users in this example) assigned to it.

4. Repeat the preceding two steps until the assignments don't change, or change very little.

#### Example

For this example, we are going to use a pre-loaded dataset called *iris*. This dataset includes the length and width of the sepal and the petal for 150 iris flowers belonging to three species of plants (setosa, virginica, versicolor).

Here is you can see the difference between a sepal and a petal:
```{r, out.width = "400px", fig.align="center"}
knitr::include_graphics("https://upload.wikimedia.org/wikipedia/commons/7/7f/Mature_flower_diagram.svg")
```

Let's start by exploring the columns in the dataset. Execute the following instruction by pressing *Run*

```{r exploreHead, exercise=TRUE}
iris
```

We are going to be using the length and width columns to try to group similar flowers into the same species. We will only use the *Species* column to compare results.

We can start by grouping the flowers using the sepal length and the petal length, and see how it goes. To start exploring the data, we can create a scatter plot where the x axis is the petal length, while the y axis is the sepal length.

**Question: Do you see any patterns in the plot?**

```{r explorePlot, exercise=TRUE}
ggplot(iris, aes(x=Petal.Length,y=Sepal.Length))+
  geom_point()
```

You might have identified that there seems to be at least two groups of flowers: small petal and sepal, and then a group with petals > 3. 

While we know the data set contains three species of plants, let's start using the k-means algorithm to see if it identifies these visible two groups 

```{r computeTwoKmeans, exercise=TRUE}
# Since it starts from a random data point, we want to set a seed to obtain the same clusters every time we run it. 
set.seed(1234)
# Now we use kmeans function passing only the relevant colums, and the number of clusters k=2
groups<- kmeans(iris[,c('Sepal.Length','Petal.Length')], 2)
# Explore the results
groups
```

#### Plot the Clusters
To plot the clusters, we just need to match the resulting clusters to our dataset, and use the colors in the scatter plot to distinguish the groups

```{r plotTwoClusters, exercise=TRUE}
iris$group<-groups$cluster
ggplot(iris, aes(x=Petal.Length,y=Sepal.Length, color=factor(group)))+
  geom_point()
```

#### Did the algorithm find the same groups you had considered?

We will now run the algorithm to find three clusters, and plot the results.

#### Practice Activity:
What is this algorithm doing? Write comments within the sample code to describe what each line does.

```{r computeTreKmeans, exercise=TRUE}
set.seed(1234)
groups<- kmeans(iris[,c('Sepal.Length','Petal.Length')], 3)
groups
iris$group<-groups$cluster
ggplot(iris, aes(x=Petal.Length,y=Sepal.Length, color=factor(group)))+
  geom_point()
```



There are now three clusters. Some of the points of the first cluster moved to the second one, which is now split around a petal length of 5. We can compare those results to the actual species:
```{r plotSpeciesKmeans, exercise=TRUE}
ggplot(iris, aes(x=Petal.Length,y=Sepal.Length, color=factor(Species)))+
  geom_point()
```

#### How did the algorithm did finding groups of flowers from the same species?

We can compute an indicator of performance by computing the number of flowers appropriately classified over the total number of flowers.


```{r prepare-indicator}
set.seed(1234)
groups<- kmeans(iris[,c('Sepal.Length','Petal.Length')], 3)
groups
iris$group<-groups$cluster
```




```{r indicatorPerformance, exercise=TRUE, exercise.setup = "prepare-indicator"}
iris$group<- factor(iris$group, levels=c(1,2,3), labels=levels(iris$Species))
performance<- 100*sum(iris$Species==iris$group)/150
```

#### Practice Activity:
Maybe we can improve that indicator of performace if we also include the sepal width and the petal width in the clustering procedure. Use the following box to find three clusters using all four columns (lengths anad widths). Compute and compare the indicator of performance to the one we just did. **Note:** Make sure you include comments within your code to explain how you solved it, and how your program performed.

```{r activityWidths, exercise=TRUE}
```

## Step-by-Step Algorithm

As we discussed in the previos section, we would like to minimize the within-cluster variation (i.e, when the data points are closest to their centroid). This variation for each cluster $C_j$ is defined by the following equation:


<div align=center>
$W(C_j)=\sum_{x_i \in C_j} (x_i-\mu_j)^2$ 
</div>



Where $x_i$ is a data point in the cluster $C_j$, and $\mu_j$ is the mean value (centroid) of a cluster $C_j$.

The total within-cluster variation, which is minimized in the algorithm, is then the sum of all the within-cluster variations:


<div align=center>
$TotalWithinCluster=\sum_{j=1}^{j=k} W(C_k)=\sum_{j=1}^{j=k}\sum_{x_i \in C_k} (x_i-\mu_k)^2$ 
</div>

#### THe first set of centroids

We start by first generating a random set of centroids. The centroids will be plotted in red.

**Note** that we set a seed to generate the random numbers. We do that to always get the same set of random numbers. However, you can change the seed and see how the random numbers change.
```{r generateCentroids, exercise=TRUE}
set.seed(123)  # Change the value of the seed and execute again to see how the inital centroid change.
initialCentroids<-sample.int(150, 3) # We simply generatae three random positions to be our randomCentroids
randomCentroids<- iris[initialCentroids,c('Sepal.Length','Petal.Length')]
# Visualize the centroids
ggplot(iris, aes(x=Petal.Length,y=Sepal.Length))+
  geom_point()+
  geom_point(data = randomCentroids, col='red')
```

```{r findClosest, exercise=TRUE}
set.seed(123)
initialCentroids<-sample.int(150, 3)
randomCentroids<- iris[initialCentroids,c('Sepal.Length','Petal.Length')]
# Compute the distance and find the closest centroid
cluster<- apply(iris, 1, FUN=computeDistance, randomCentroids=randomCentroids)
# assign the group
iris$group<-cluster
ggplot(iris, aes(x=Petal.Length,y=Sepal.Length, color=factor(group)))+
  geom_point()+
  geom_point(data = randomCentroids, col='red')
```


```{r prepare-iris}
set.seed(123)
initialCentroids<-sample.int(150, 3)
randomCentroids<- iris[initialCentroids,c('Sepal.Length','Petal.Length')]
# Compute the distance and find the closest centroid
cluster<- apply(iris, 1, FUN=computeDistance, randomCentroids=randomCentroids)
# assign the group
iris$group<-cluster
```



We now want to find the new centroids. Update the variable *numIter* for a number larger than 1 so you can see how the groups change
```{r iterate, exercise=TRUE, exercise.setup = "prepare-iris"}
numIter=2
dataToPlot<-iris[,c('Sepal.Length','Petal.Length')]
randomCentroids$group<--1
randomCentroids$iteration<-0
dataToPlot$group<-1 
dataToPlot$iteration<-0
dataToPlot<-rbind(dataToPlot,randomCentroids)
for(i in 0:numIter) {
  # Compute the new centroids
  randomCentroids[1,]<-c(colMeans(iris[iris$group==1,c('Sepal.Length','Petal.Length')]),-1,i)
  randomCentroids[2,]<-c(colMeans(iris[iris$group==2,c('Sepal.Length','Petal.Length')]),-1,i)
  randomCentroids[3,]<-c(colMeans(iris[iris$group==3,c('Sepal.Length','Petal.Length')]),-1,i)
  
  # Compute the distances to the new centroids, and identify what cluster each datapoint belongs to
  cluster<- apply(iris, 1, FUN=computeDistance, randomCentroids=randomCentroids[,1:2])
  iris$group<-cluster
  iris$iteration<-i
  dataToPlot<-rbind(dataToPlot,
                    iris[,c('Sepal.Length','Petal.Length','group','iteration')],
                    randomCentroids)
  
}
dataToPlot$group<- factor(dataToPlot$group, levels=c(-1,1,2,3), labels=c('Centroid', 'Group 1', 'Group 2', 'Group 3'))
ggplot(dataToPlot, aes(x=Petal.Length,y=Sepal.Length, color=group))+
  geom_point()+
  facet_grid(vars(factor(iteration)))
```
